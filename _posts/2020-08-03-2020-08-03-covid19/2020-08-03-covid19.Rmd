---
name: "Aspire Data Solutions"
title: "Applying Bibliometric analysis and text mining to COVID-19 publications?"
author: Mihiretu Kebede(PhD)
url: http://www.mihiretukebede.com/ 
google_analytics: "UA-173027539-1"
date: "2020-08-04`"
description: |
  Let's have a quick overview of what on COVID-19 is just indexed in PubMed only in August 03/2020.

collections:
  posts:
      share: [twitter, linkedin, facebook, google-plus, pinterest]
      
output:
  distill::distill_article:
    self_contained: false
    widerscreen: true
---


# Plan of attack
## Source of data 
- Let's do a quick PubMed search
copy the following and search on PubMed.
(covid 19) AND (("2020/08/02"[Date - Publication] : "3000"[Date - Publication]))

- With the above search terms, we have 601 COVID-19 related records were indexed in PubMed 
- Alternatively you can find the search link [here](https://pubmed.ncbi.nlm.nih.gov/?term=%28covid+19%29+AND+%28%28%222020%2F07%2F31%22%5BDate+-+Publication%5D+%3A+%223000%22%5BDate+-+Publication%5D%29%29).

- We can now open these records using our citation manager. 
- Since the file is in RIS format, we must convert it to BibTeX so that we can be be able to benefit from **Bibliomterix** package. 
- I will come back in the future in another blog post to show you on how to convert RIS file to BibTeX or from CSV/XSL to EndNote-readable format. For today, let's just use the converted [COVID19.bib](https://github.com/Mihiretukebede/mihiretukebede.gitlub.io/blob/master/posts/2020-08-03-2020-08-03-covid19/covid19.bib)


## Initialize
Set working directory

## Load necessary packages

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
library(bibliometrix) #for bibliometric analysis
library(dplyr) #for data management
library(ggplot2) #for plotting
library(tidytext) #for text mining
library(bib2df) #for converting bib file to data frame
```

# Import the data to R
In another blog post, I will come back to show you how to convert RIS reference file to BibTeX of csv file to EndNote readable file formats. For now, let's first use *bibliometrix* and then *bib2df* package to convert our [covid bib file](https://github.com/Mihiretukebede/mihiretukebede.gitlub.io/blob/master/posts/2020-08-03-2020-08-03-covid19/covid19.bib) to data frame. 


# Covert the BibTeX file to data frame

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
covid19_bibanalysis <- convert2df("covid19.bib", dbsource = "isi", format = "bibtex")

#As usual, isnpect the data
dim(covid19_bibanalysis) #601 records, 23 variables
glimpse(covid19_bibanalysis) #Inspect the structure of data, variable names, etc

```

# Now bibliometrics analysis

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
results <-  biblioAnalysis(covid19_bibanalysis, sep = ",") #create the object
options(width=100) #to determine width of the plot

s <- summary(object=results, k=10, pase=FALSE) #to present 10 most prominent authors, journals, keywords, etc

plot(x=results, k=10, pause=FALSE) #plot the results

#since all of them are indexed in August 2020, this may not be useful analysis. 

#Let's see sankey plots. But, before that we must remove missing values
threeFieldsPlot(covid19_bibanalysis, fields = c("AU", "DE", "SO"))

#Look at this plot. It maps Authors with keywords and with the journals. With 61 articles indexed in PubMed in one day, a journal called, EMERGING MICROBES \\& INFECTIONS  is leading the league


## Now let's do some clustering
cs <- conceptualStructure(covid19_bibanalysis, field="AB", 
                          method="CA", minDegree = 4,
                          stemming=FALSE, labelsize = 10, documents=2) 
# is abit dense to vizualize it here
```

# Now, let's use bib2df

It is also possible to continue using the `covid19_bibanalysis`data. However, it is also good to try another package, `bib2df`. We will use this data frame for our text mining analysis. Sometimes, the data frame converted using *bibliometrix*  package may not also work properly with *dplyr*.  

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
library(bib2df)
covid19 <- bib2df("covid19.bib")
```


# Inspect the data

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
glimpse(covid19) 
#Let's choose only some of our variabees

covid19data <- covid19 %>% 
  select("AUTHOR", "TITLE", "KEYWORDS", "ABSTRACT", "JOURNAL","DOI")

dim(covid19) #601 records, 23 variables
which(!complete.cases(covid19$DOI)) # All of them have missing values
sum(is.na(covid19$ABSTRACT)) #159 studies have no abstract
sum(is.na(covid19$TITLE)) # 0 records have missing values
sum(is.na(covid19$DOI)) #2 studies have no doi
sum(!is.na(covid19$ABSTRACT)) #442 records have abstracts

#Let's filter the data to retreive only the records with Abstract(AB)

```

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
#Let's filter the data to retreive only the records with Abstract(AB)
#As we have seen above, we have 159 records without abstracts
#use dplyr to filter
library(dplyr)
covid19new <- covid19 %>%
  filter(!is.na(ABSTRACT)) #remove all records with missing abstracts
sum(!is.na(covid19new$ABSTRACT)) #442 records have abstracts
   #BINGO, we excluded the studies without Abstract. Now we, can play with text mining. 

```

# Text mining
- Since we already have our 442 studies with no missing abstract, we can conduct our text mining analysis using this data. 
- There are several r packages for conducting text mining. Most popular packages are *tidytextt*, *text2vec*, *quanteda*, *tm* and *stringr*. 
- For today, I will simply follow  *tidytext*.

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
# selct few variables from our data set
data <- covid19new %>% 
  select("TITLE", "ABSTRACT", "KEYWORDS", "AUTHOR")

```
# Tokenization
- Tokenization is the process of breaking a certain text into word by word columns. For example, if one abstract is written using 300 words, tokenizing the abstract will result in 300 columns for each word. This will make things easy to count words and do any further analysis.    
```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
tidy_covid_data <- data %>% 
  unnest_tokens(input=ABSTRACT, output=word)
```
# Remove stop words
Stop words are words that are not very relevant to the meaning or concept of the document. For example: "The COVID19 pandemic is the biggest global health crisis of our time". In this sentence,  "the, is, of, our" are not relevant for the concept of this text. These words need to be removed from our analys.  
```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
data("stop_words")
tidy_covid_data <- tidy_covid_data %>% 
  anti_join(stop_words)
```

# Customize stop_words

- Usually, abstract contains words like "background", "introduction", "materials", "methods", "results", "conclusions", etc. Let's remove these words 

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
# since it is all about covid-19, we don't need covid 
custom_stop_words <- bind_rows(tibble(word=c("covid", "cov", "background", "introduction", "materials", "methods", "results", "conclusions", "0", "1","2", "3", "4", "5", "6", "7", "8", "9", "2019", "20202"),
                                      lexicon = c("custom")),
                               stop_words)

# count number of words and plot it
 tidy_covid_data %>% 
  count(word, sort=T) %>% 
   filter(n>150) %>% 
   mutate(word=reorder(word, n)) %>% 
   ggplot(aes(x=word, y=n)) + 
   geom_col() + coord_flip()
```

# Word cloud

```{r, cache=T, include=T, echo=T, message=FALSE, warning=FALSE}
library(wordcloud)
pal <- brewer.pal(8, "Dark2")
tidy_covid_data %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words=300, colors = pal))
```


# Contact 
Please mention [MihiretuKebede1](https://www.https://twitter.com/MihiretuKebede1) if you tweet this post. 


